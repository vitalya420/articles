# DDoS HTTP пакетами для чайників

Ця стаття допоможе зрозуміти, як сервери опрацьовують HTTP-запити, і пояснить, як надмірна кількість запитів може призвести до відмови в обслуговуванні.


## Забити мережевий трафік
На перший погляд, здається, що найпростіший спосіб задудосити сервер — це **забити мережевий трафік** до такої міри, що нові пакети почнуть оброблятися дуже повільно або зовсім відхилятися. Це має сенс, адже встановлення кожного нового з'єднання та **SSL Handshake** потребують обміну кількома пакетами між клієнтом і сервером.  

Але тут є нюанс.  
Сучасні сервери в датацентрах мають пропускну здатність близько **10 гігабіт на секунду** або навіть більше. Для того щоб вичерпати таку пропускну здатність, потрібна неймовірно велика кількість запитів на секунду.  

Куди ефективніше — навантажувати вихідну пропускну здатність сервера, адже кожна відповідь може бути значно важчою за вхідний запит. Наприклад, звичайний запит `GET / HTTP/1.1\r\n\r\n` розміром 18 байт, може згенерувати відповідь в декілька кілобайт.

## Вичерпення Worker'ів
На веб-сервері майже завжди стоїть **reverse proxy**, наприклад, **Nginx** (один із найпопулярніших), **Apache HTTP Server (httpd)** чи інші. Основна функція reverse proxy — приймати вхідні запити та передавати їх далі на бекенд.  

Але кількість запитів, які reverse proxy може обробити одночасно, **не є безмежною**.  
Кожне з'єднання обслуговується окремим воркером або потоком, і їхня кількість обмежена налаштуваннями сервера та системними ресурсами.  

При надмірній кількості з'єднань сервер просто **не зможе виділити новий воркер**, і нові запити залишаться без обробки. Це призведе до **відмови в обслуговуванні** для нових користувачів.  


## Вичерпання кількості портів.
Коли запит доходить до **Reverse Proxy**, він перенаправляється на бекенд-сервер. Важливо розуміти, що бекенд отримує з'єднання не від кінцевого користувача, а від локальної адреси, наприклад, **10.0.0.2**. Ця адреса може бути частиною внутрішньої мережі Docker або Kubernetes.

І тут виникає **проблема 65 тисяч з'єднань**. 

Операційна система має обмеження на кількість одночасних з'єднань, яке зазвичай становить **65 536 портів** для кожної IP-адреси. Якщо всі порти будуть зайняті, нові з'єднання просто не зможуть бути встановлені, і сервер перестане відповідати. У випадку з внутрішньою мережею це обмеження може стати серйозною вразливістю.

Ця проблема, звичайно, вирішується досить просто — використанням **UNIX-сокетів** замість TCP-з'єднань. UNIX-сокети не мають обмеження на кількість портів, що дозволяє уникнути ліміту в 65 тисяч з'єднань.

Проте навіть при використанні UNIX-сокетів серверу все одно потрібні **системні виклики**, щоб передати кожен запит. І саме на це ми будемо тиснути під час атаки — системні виклики споживають ресурси процесора, що в кінцевому підсумку може призвести до вичерпання ресурсів і відмови в обслуговуванні.

## Вичерпення оперативної пам'яті.

Уявіть ситуацію: **100 з'єднань** відправляють по **1 мегабайту даних**. Сервер повинен десь зберігати ці дані, і зазвичай він робить це в **оперативній пам'яті**. На цьому етапі все ще виглядає прийнятно.

Але тепер уявіть, що кількість з'єднань зростає до **1000**, і кожне з них передає по **5 мегабайт даних**. Це вже **~4 гігабайти пам'яті**! Якщо оперативна пам’ять закінчується, у Linux виникає **Out of Memory (OOM)** ситуація.  

І як діє Linux у цьому випадку?  
Він просто **вбиває процес**, який споживає найбільше пам’яті, щоб звільнити ресурси. У нашому випадку це буде веб-сервер або бекенд-процес, що призводить до **повного припинення обслуговування**. Однак, супервізор процес перезапустить.

## Потокове голодування.
Ще однією серйозною проблемою є **потокове голодування**. Якщо велика кількість одночасних запитів передається на бекенд, серверу може просто **не вистачити ресурсів** для обробки всіх цих запитів.  

У такій ситуації кожен запит вимагає обробки, але через обмежені ресурси **операційна система починає постійно перемикати контекст** між потоками. Це перемикання є **дуже дорогим завданням**.

У результаті **більша частина часу витрачається на перемикання контексту**, а не на реальну обробку запитів. Це знижує продуктивність і може призвести до повного зависання сервера.  

Як створити велику кількість запитів одночасно — про це згодом.

## Обмеження в кількості системних викликів.
Уявімо таку ситуацію: клієнт надсилає запит на сервер, щоб авторизуватися. Давайте порахуємо, що потрібно зробити клієнту:  

1. Встановити з’єднання.  
2. Виконати **SSL handshake**.  
3. Записати запит у **socket**.  
4. Дочекатися відповіді від сервера.  

А тепер розглянемо, що повинен зробити сервер у відповідь:  

1. **Прийняти з’єднання**.  
2. Виконати **SSL handshake**.  
3. **epoll** для моніторингу з’єднання.  
4. Відкрити з’єднання з бекендом і передати запит.  
5. Бекенд має **прочитати запит** і з’єднатися з базою даних.  
6. База даних виконує **читання або запис**, після чого бекенд формує відповідь.  

Як бачимо, **кількість системних викликів на сервері значно більша**, ніж на стороні клієнта. І хоча сучасні процесори здатні обробляти величезну кількість системних викликів, це не безмежний ресурс.  

**При великому навантаженні навіть найпотужніший процесор може "лягти"**, вичерпавши можливості обробки системних викликів у секунду.  

## "Удар по гаманцю"
Хостинг-провайдери, такі як **Google Cloud**, **AWS**, **DigitalOcean** та інші, мають **квоти на вихідний трафік**. Наприклад, у DigitalOcean додаткова плата за перевищення квоти становить **$0.01 за 1 гігабайт**.  

Як це можна використати? Достатньо знайти **великий файл** на сервері та **завантажувати його багато разів**. Таким чином, ми швидко вичерпаємо квоту на вихідний трафік.  

**Досвід та експерименти показують**, що перевищити цю квоту можна **дуже швидко**, завдавши фінансових збитків власнику сервера.  

## Визначення конфігурації сервера
Важливо визначити такі параметри, як **connection timeout**, **client max body** та кількість запитів, які можуть бути передані в одному з'єднанні (через **Connection: keep-alive**).

### 1. Connection Timeout

Знаючи значення **connection timeout**, можна спланувати стратегію для відкриття великої кількості з'єднань. Наприклад, якщо тайм-аут складає **1 хвилину**, можна відправити запит за секунду до того, як сервер закриє з'єднання. В цей час можна відкривати ще сотні або тисячі з'єднань. Але варто пам'ятати про обмеження на кількість з'єднань, адже **65 тисяч портів** з однієї IP-адреси — це ліміт.

### 2. Client Max Body
Другим важливим параметром є **client max body**. Знаючи максимальний розмір тіла запиту, можна навантажити сервер, відправляючи найбільші можливі файли чи дані, поки сервер не закриє з'єднання через перевищення цього ліміту. І тут варто також враховувати **connection timeout** для максимального використання доступного часу.

### 3. Тестування навантаження на процесор
Інший варіант — це відправлення великого **JSON** об'єкта для використання **процесорного часу на парсинг**. Знаючи, як довго сервер обробляє цей запит, можна визначити кількість доступних ядер процесора. Як це зробити? Просто!

1. Спочатку відправляємо **JSON**, який парситься протягом, наприклад, 2 секунд.
2. Потім відправляємо цей самий JSON у паралельних з'єднаннях і дивимося, чи збільшиться час обробки. Якщо час зростає, це може означати, що серверу не вистачає ядер процесора для паралельної обробки запитів.

### Важливість
Знаючи конфігурацію сервера та ресурси хостинг-провайдера, можна сформувати більш ефективний план для атаки або для оцінки стійкості сервера до таких навантажень.


## Синхронізація по останньому байту (Last Byte Sync)

Синхронізація по останньому байту зазвичай використовується для створення гонки умов в API. Про гонку умов буде окрема стаття. В нашому випадку, ми будемо використовувати цей метод для того, щоб передати більшу кількість запитів на бекенд в один момент.

### Як це працює?

Для прикладу візьмемо звичайний запит: `GET / HTTP/1.1\r\n`

Відправивши цей запит, окрім останнього байта `\n`, тобто `GET / HTTP/1.1\r`, сервер буде вважати, що запит не завершений і чекатиме на нього якийсь час, перед тим, як закрити з'єднання. Тобто, можна відправити велику кількість незавершених запитів, і як тільки всі вони будуть відправлені, просто додати останній байт `\n`. Це займе мізерну кількість часу, і сервер почне опрацьовувати всі запити одночасно.

### У випадку з POST запитом

В POST запиті міститься хедер `content-length`, який вказує кількість байтів в тілі запиту. Тут аналогічно, відправляємо всі байти, окрім останнього, а коли всі вони будуть відправлені, то відправляємо останній байт. 

```
POST / HTTP/1.1\r\n
content-length: 17\r\n
\r\n
{"hello":"world"}
```

Тут останній байт — це `}`.

